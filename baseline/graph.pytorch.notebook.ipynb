{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STATIC GRAPH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torch.autograd import Variable\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import data_utils\n",
    "metadata, idx_q, idx_a = data_utils.load_data('../data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# add special symbol\n",
    "i2w = metadata['idx2w'] + ['GO']\n",
    "w2i = metadata['w2idx']\n",
    "w2i['GO'] = len(i2w)-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "B = 256\n",
    "L = len(idx_q[0])\n",
    "vocab_size = len(i2w)\n",
    "enc_hdim = 250\n",
    "dec_hdim = enc_hdim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "130422"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(idx_q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, batch_size, vocab_size, hidden_size, input_size):\n",
    "        super(Model, self).__init__()\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.input_size  = input_size\n",
    "        \n",
    "        self.encode = nn.LSTMCell(hidden_size, hidden_size)\n",
    "        self.decode = nn.LSTMCell(hidden_size, hidden_size)\n",
    "        \n",
    "        self.embed = nn.Embedding(vocab_size, hidden_size)\n",
    "        self.project = nn.Linear(hidden_size, vocab_size)\n",
    "        \n",
    "        self.printsize = True\n",
    "        \n",
    "    def initial_state(self):\n",
    "        state = torch.zeros([self.batch_size, self.hidden_size])\n",
    "        return Variable(state.cuda())\n",
    "    \n",
    "    def psize(self, name, tensor):\n",
    "        if self.printsize:\n",
    "            print(name, tensor.size(), type(tensor))\n",
    "    \n",
    "    def forward(self, enc_inputs, dec_outputs):\n",
    "        self.psize('enc_inputs', enc_inputs)\n",
    "        self.psize('dec_outputs', dec_outputs)\n",
    "\n",
    "        enc_embeddings = self.embed(enc_inputs)                                 #BxLXH <- BxL\n",
    "        dec_embeddings = self.embed(dec_outputs)                                #BXLXH <- BxL\n",
    "        \n",
    "        self.psize('enc_embeddings', enc_embeddings)\n",
    "        self.psize('dec_embeddings', dec_embeddings)\n",
    "        \n",
    "        hidden = cell_state = self.initial_state()                              #BxH\n",
    "        for i in range(enc_embeddings.size()[1]):\n",
    "            hidden, cell_state = self.encode(enc_embeddings[:,i], (hidden, cell_state))\n",
    "        \n",
    "        self.psize('hidden', hidden)\n",
    "        self.psize('cell_state', cell_state)\n",
    "        \n",
    "        predicted_outputs = []\n",
    "        first_input = torch.LongTensor([w2i['GO']] * self.batch_size).cuda()\n",
    "            \n",
    "        first_input = Variable(first_input)\n",
    "        self.psize('first_input', first_input)\n",
    "        first_input = self.embed(first_input)\n",
    "        self.psize('first_input', first_input)\n",
    "        hidden, cell_state = self.decode(first_input, (hidden, cell_state))\n",
    "        predicted_outputs.append(hidden)\n",
    "        for i in range(dec_embeddings.size()[1] - 1):\n",
    "            import random\n",
    "            if random.random():\n",
    "                hidden, cell_state = self.decode(dec_embeddings[:,i+1], (hidden, cell_state))\n",
    "            else:\n",
    "                hidden, cell_state = self.decode(dec_embeddings[:,i+1], (hidden, cell_state))\n",
    "                \n",
    "            predicted_outputs.append(hidden)\n",
    "            \n",
    "        predicted_outputs = torch.stack(predicted_outputs)\n",
    "        predicted_outputs = predicted_outputs.view(\n",
    "                            self.batch_size * dec_outputs.size()[1], \n",
    "                            self.hidden_size)\n",
    "        self.psize('predicted_outputs', predicted_outputs)\n",
    "        outputs = self.project(predicted_outputs)\n",
    "        self.psize('outputs', outputs)\n",
    "        outputs = outputs.view(self.batch_size, dec_outputs.size()[1], self.vocab_size)\n",
    "        self.psize('outputs', outputs)\n",
    "        \n",
    "        outputs = F.log_softmax(outputs)\n",
    "        #outputs = F.log_softmax(outputs).max(2)[1].squeeze(2)\n",
    "        self.psize('outputs', outputs)\n",
    "\n",
    "        return outputs\n",
    "    \n",
    "    def predict(self, enc_inputs):\n",
    "        self.psize('enc_inputs', enc_inputs)\n",
    "\n",
    "        enc_embeddings = self.embed(enc_inputs)                                 #BxLXH <- BxL\n",
    "        hidden = cell_state = self.initial_state()                              #BxH\n",
    "        for i in range(enc_embeddings.size()[1]):\n",
    "            hidden, cell_state = self.encode(enc_embeddings[:,i], (hidden, cell_state))\n",
    "\n",
    "        predicted_outputs = []\n",
    "        first_input = torch.LongTensor([w2i['GO']] * self.batch_size).cuda()\n",
    "        first_input = Variable(first_input)\n",
    "        first_input = self.embed(first_input)\n",
    "\n",
    "        hidden, cell_state = self.decode(first_input, (hidden, cell_state))\n",
    "        predicted_outputs.append(hidden)\n",
    "        for i in range(enc_embeddings.size()[1] - 1):\n",
    "            hidden, cell_state = self.decode(hidden, (hidden, cell_state))\n",
    "            predicted_outputs.append(hidden)\n",
    "            \n",
    "        predicted_outputs = torch.stack(predicted_outputs)\n",
    "        predicted_outputs = predicted_outputs.view(\n",
    "                                self.batch_size * enc_inputs.size()[1], \n",
    "                                self.hidden_size)\n",
    "        \n",
    "        outputs = F.tanh(self.project(predicted_outputs))\n",
    "        \n",
    "        outputs = outputs.view(self.batch_size, enc_inputs.size()[1], self.vocab_size)\n",
    "        outputs = F.log_softmax(outputs).max(2)[1].squeeze(2)\n",
    "        self.psize('outputs', outputs)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "def train(epochs, model, question_ids, answer_ids):\n",
    "    model.train()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.1)\n",
    "    batches = len(question_ids)//B\n",
    "    print('number of batches:', batches)\n",
    "    for epoch in range(epochs+1):\n",
    "        avg_loss = 0\n",
    "        for batch in range(batches):\n",
    "            l, r = batch * B, (batch + 1) * B\n",
    "            data = Variable(torch.from_numpy(question_ids[l:r]).long().cuda())\n",
    "            target = Variable(torch.from_numpy(answer_ids[l:r]).long().cuda())\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            logits = model(data, target)\n",
    "            b, l = target.size()\n",
    "            target = target.view(B * l)\n",
    "            logits = logits.view(B*l, -1)\n",
    "            \n",
    "            loss = F.nll_loss(logits, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            avg_loss += loss.data[0]\n",
    "            model.printsize = False\n",
    "            if not batch % 20 and batch:\n",
    "                print('{}.{} - {} {}'.format(epoch, batch, avg_loss/20, loss.data[0]))\n",
    "                avg_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Model(B, vocab_size, enc_hdim, 1).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of batches: 509\n",
      "enc_inputs torch.Size([256, 20]) <class 'torch.autograd.variable.Variable'>\n",
      "dec_outputs torch.Size([256, 20]) <class 'torch.autograd.variable.Variable'>\n",
      "enc_embeddings torch.Size([256, 20, 250]) <class 'torch.autograd.variable.Variable'>\n",
      "dec_embeddings torch.Size([256, 20, 250]) <class 'torch.autograd.variable.Variable'>\n",
      "hidden torch.Size([256, 250]) <class 'torch.autograd.variable.Variable'>\n",
      "cell_state torch.Size([256, 250]) <class 'torch.autograd.variable.Variable'>\n",
      "first_input torch.Size([256]) <class 'torch.autograd.variable.Variable'>\n",
      "first_input torch.Size([256, 250]) <class 'torch.autograd.variable.Variable'>\n",
      "predicted_outputs torch.Size([5120, 250]) <class 'torch.autograd.variable.Variable'>\n",
      "outputs torch.Size([5120, 8003]) <class 'torch.autograd.variable.Variable'>\n",
      "outputs torch.Size([256, 20, 8003]) <class 'torch.autograd.variable.Variable'>\n",
      "outputs torch.Size([256, 20, 8003]) <class 'torch.autograd.variable.Variable'>\n",
      "0.20 - 5.825427603721619 5.546547889709473\n",
      "0.40 - 5.547290468215943 5.545630931854248\n",
      "0.60 - 5.547361850738525 5.547147750854492\n",
      "0.80 - 5.547456097602844 5.547529697418213\n",
      "0.100 - 5.547088289260865 5.546292304992676\n",
      "0.120 - 5.546969938278198 5.5475029945373535\n",
      "0.140 - 5.546877217292786 5.546745300292969\n",
      "0.160 - 5.5465278148651125 5.547080039978027\n",
      "0.180 - 5.54666588306427 5.547915458679199\n",
      "0.200 - 5.546770668029785 5.547747611999512\n",
      "0.220 - 5.546637463569641 5.546494483947754\n",
      "0.240 - 5.546488356590271 5.546621322631836\n",
      "0.260 - 5.546776676177979 5.5474138259887695\n",
      "0.280 - 5.546495461463929 5.5461015701293945\n",
      "0.300 - 5.546510004997254 5.545622825622559\n",
      "0.320 - 5.546769857406616 5.546286582946777\n",
      "0.340 - 5.546278095245361 5.546654224395752\n",
      "0.360 - 5.546427464485168 5.546897888183594\n",
      "0.380 - 5.546474099159241 5.546917915344238\n",
      "0.400 - 5.546320199966431 5.5452799797058105\n",
      "0.420 - 5.546770167350769 5.546350002288818\n",
      "0.440 - 5.546380114555359 5.5450897216796875\n",
      "0.460 - 5.5464294195175174 5.547133445739746\n",
      "0.480 - 5.546175312995911 5.5459160804748535\n",
      "0.500 - 5.546151518821716 5.545653343200684\n",
      "1.20 - 8.0425035238266 5.545018672943115\n",
      "1.40 - 5.546341681480408 5.545077323913574\n",
      "1.60 - 5.546470212936401 5.546306610107422\n",
      "1.80 - 5.54658362865448 5.546425819396973\n",
      "1.100 - 5.546393394470215 5.545600891113281\n",
      "1.120 - 5.546325254440307 5.547048568725586\n",
      "1.140 - 5.5463121891021725 5.546292304992676\n",
      "1.160 - 5.545935535430909 5.546200752258301\n",
      "1.180 - 5.546129369735718 5.5473127365112305\n",
      "1.200 - 5.546316051483155 5.547488212585449\n",
      "1.220 - 5.546133494377136 5.545829772949219\n",
      "1.240 - 5.546035861968994 5.546163558959961\n",
      "1.260 - 5.5463111162185665 5.546849727630615\n",
      "1.280 - 5.546096634864807 5.545748710632324\n",
      "1.300 - 5.546126341819763 5.545435905456543\n",
      "1.320 - 5.54628050327301 5.545914649963379\n",
      "1.340 - 5.545982408523559 5.546159267425537\n",
      "1.360 - 5.546044111251831 5.546377658843994\n",
      "1.380 - 5.5461020231246945 5.5464630126953125\n",
      "1.400 - 5.545992922782898 5.54518985748291\n",
      "1.420 - 5.546388006210327 5.545962810516357\n",
      "1.440 - 5.546077537536621 5.544956207275391\n",
      "1.460 - 5.5460898876190186 5.546578884124756\n",
      "1.480 - 5.545895314216613 5.545754909515381\n",
      "1.500 - 5.545828485488892 5.545424461364746\n",
      "2.20 - 8.042062640190125 5.544857025146484\n",
      "2.40 - 5.546058106422424 5.544989585876465\n",
      "2.60 - 5.546179580688476 5.54609489440918\n",
      "2.80 - 5.5462569952011105 5.546021461486816\n",
      "2.100 - 5.546135878562927 5.545395851135254\n",
      "2.120 - 5.546073722839355 5.546750068664551\n",
      "2.140 - 5.546095848083496 5.546039581298828\n",
      "2.160 - 5.545706248283386 5.5458664894104\n",
      "2.180 - 5.545903110504151 5.547001838684082\n",
      "2.200 - 5.546113276481629 5.54738712310791\n",
      "2.220 - 5.54590871334076 5.545565605163574\n",
      "2.240 - 5.5458495140075685 5.5460405349731445\n",
      "2.260 - 5.546085834503174 5.546548366546631\n",
      "2.280 - 5.545915031433106 5.545604705810547\n",
      "2.300 - 5.545917010307312 5.545375823974609\n",
      "2.320 - 5.546033978462219 5.545734405517578\n",
      "2.340 - 5.5458251476287845 5.545884132385254\n",
      "2.360 - 5.5458485841751095 5.546137809753418\n",
      "2.380 - 5.545897722244263 5.546200752258301\n",
      "2.400 - 5.5458109140396115 5.545121192932129\n",
      "2.420 - 5.5461636781692505 5.545751571655273\n",
      "2.440 - 5.545902490615845 5.544900417327881\n",
      "2.460 - 5.545895767211914 5.546282768249512\n",
      "2.480 - 5.545725774765015 5.545650005340576\n",
      "2.500 - 5.545644402503967 5.545313358306885\n",
      "3.20 - 8.041807198524475 5.544777870178223\n",
      "3.40 - 5.545883846282959 5.544951438903809\n",
      "3.60 - 5.545996522903442 5.545971393585205\n",
      "3.80 - 5.54605929851532 5.545784950256348\n",
      "3.100 - 5.545975136756897 5.545256614685059\n",
      "3.120 - 5.545912027359009 5.546530246734619\n",
      "3.140 - 5.545959520339966 5.5458574295043945\n",
      "3.160 - 5.545561718940735 5.545684337615967\n",
      "3.180 - 5.545759582519532 5.546802520751953\n",
      "3.200 - 5.54598217010498 5.547283172607422\n",
      "3.220 - 5.545768165588379 5.545409679412842\n",
      "3.240 - 5.545732712745666 5.545965194702148\n",
      "3.260 - 5.545933103561401 5.546338081359863\n",
      "3.280 - 5.545793509483337 5.545512676239014\n",
      "3.300 - 5.545772409439087 5.545368194580078\n",
      "3.320 - 5.54587140083313 5.545611381530762\n",
      "3.340 - 5.545713305473328 5.5456976890563965\n",
      "3.360 - 5.545715379714966 5.54598331451416\n",
      "3.380 - 5.545756530761719 5.5460100173950195\n",
      "3.400 - 5.545681190490723 5.545059680938721\n",
      "3.420 - 5.546004414558411 5.545595169067383\n",
      "3.440 - 5.545776987075806 5.54486083984375\n",
      "3.460 - 5.5457580327987674 5.546088218688965\n",
      "3.480 - 5.5456032991409305 5.545566082000732\n",
      "3.500 - 5.545515036582946 5.545236587524414\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-90-c3eefd453a78>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx_q\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx_a\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-86-a7317e053429>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epochs, model, question_ids, answer_ids)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/paarulakan/environments/python/pytorch-py35/lib/python3.5/site-packages/torch/autograd/variable.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_variables)\u001b[0m\n\u001b[1;32m    144\u001b[0m                     'or with gradient w.r.t. the variable')\n\u001b[1;32m    145\u001b[0m             \u001b[0mgradient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize_as_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_execution_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(10, model, idx_q, idx_a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_q torch.Size([256, 20]) <class 'torch.autograd.variable.Variable'>\n",
      "enc_inputs torch.Size([256, 20]) <class 'torch.autograd.variable.Variable'>\n",
      "outputs torch.Size([256, 20]) <class 'torch.autograd.variable.Variable'>\n"
     ]
    }
   ],
   "source": [
    "batch = 0\n",
    "l, r = batch * B, (batch + 1) * B\n",
    "test_q, test_a = idx_q[l:r], idx_a[l:r]\n",
    "model.eval()\n",
    "test_q = Variable(torch.from_numpy(test_q).long().cuda())\n",
    "model.printsize = True\n",
    "model.psize('test_q', test_q)\n",
    "predictions = model.predict(test_q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def arr2sent(arr):\n",
    "    return ' '.join([i2w[item] for item in arr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('famous state famous famous reveals famous x famous famous famous famous famous code famous url seattle famous code famous reveals',\n",
       " 'how do you do this _ _ _ _ _ _ _ _ _ _ _ _ _ _ _')"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr2sent(predictions[0].cpu().data.numpy()), arr2sent(test_a[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
