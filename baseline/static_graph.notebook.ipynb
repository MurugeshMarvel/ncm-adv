{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STATIC GRAPH
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib.layers import xavier_initializer as xinit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [x] Dynamic batch size\n",
    "- [x] Mask padding while calculating loss\n",
    "- [x] Deal with variable length input sequence\n",
    "- [x] Add `EOS` (end of sentence) token to data (target sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import data_utils\n",
    "metadata, idx_q, idx_a = data_utils.load_data('../data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# add special symbol\n",
    "i2w = metadata['idx2w']\n",
    "w2i = metadata['w2idx']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "B = 8\n",
    "L = len(idx_q[0])\n",
    "vocab_size = len(i2w)\n",
    "enc_hdim = 150\n",
    "dec_hdim = enc_hdim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Placeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inputs = tf.placeholder(tf.int32, shape=[None,L], name='inputs')\n",
    "targets = tf.placeholder(tf.int32, shape=[None,L], name='targets')\n",
    "go_token = tf.reshape(tf.fill(tf.shape(inputs[:,0]), w2i['GO']), [-1,1])\n",
    "decoder_inputs = tf.concat(\n",
    "    values=[go_token, targets[:, 1:]],\n",
    "    axis=1)\n",
    "training = tf.placeholder(tf.bool, name='is_training')\n",
    "batch_size = tf.shape(inputs)[0] # infer batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor 'concat:0' shape=(?, 21) dtype=int32>,\n",
       " <tf.Tensor 'targets:0' shape=(?, 21) dtype=int32>,\n",
       " <tf.Tensor 'inputs:0' shape=(?, 21) dtype=int32>,\n",
       " <tf.Tensor 'strided_slice_2:0' shape=() dtype=int32>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_inputs, targets, inputs, batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequence Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Encoder\n",
    "#  Get sequence lengths\n",
    "def seq_len(t):\n",
    "    return tf.reduce_sum(tf.cast(t>0, tf.int32), axis=1)\n",
    "\n",
    "enc_seq_len = seq_len(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Decoder\n",
    "#   Mask targets\n",
    "#    to be applied to cross entropy loss\n",
    "padding_mask = tf.cast(targets > 0, tf.float32 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emb_mat = tf.get_variable('emb', shape=[vocab_size, enc_hdim], dtype=tf.float32, \n",
    "                         initializer=xinit())\n",
    "emb_enc_inputs = tf.nn.embedding_lookup(emb_mat, inputs)\n",
    "emb_dec_inputs = tf.nn.embedding_lookup(emb_mat, decoder_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope('encoder'):\n",
    "    encoder_cell = tf.contrib.rnn.LSTMCell(num_units=enc_hdim)\n",
    "    encoder_outputs, context = tf.nn.dynamic_rnn(cell=encoder_cell, # encoder cell\n",
    "                          inputs=emb_enc_inputs, # embedding input sequences\n",
    "                          sequence_length=enc_seq_len, # lengths of sequences\n",
    "                          initial_state = # zero state of cell\n",
    "                          encoder_cell.zero_state(batch_size=batch_size, dtype=tf.float32)\n",
    "                         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# inference - custom rnn\n",
    "def inference_decoder(cell, state):\n",
    "    #tf.get_variable_scope().reuse_variables()\n",
    "    with tf.variable_scope('decoder_1'):\n",
    "        dec_outputs = []\n",
    "        #input_ = tf.constant(w2i['GO'], dtype=tf.int32, shape=[B,])\n",
    "        input_ = tf.nn.embedding_lookup(emb_mat, tf.squeeze(go_token))\n",
    "        input_.set_shape([None, dec_hdim])\n",
    "        \n",
    "        for i in range(L):\n",
    "            decoder_output, state = cell(input_, state)\n",
    "            dec_outputs.append(decoder_output)\n",
    "            input_ = decoder_output\n",
    "\n",
    "    return tf.stack(dec_outputs), None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope('decoder') as scope:\n",
    "    decoder_cell = tf.contrib.rnn.LSTMCell(num_units=dec_hdim)\n",
    "    \n",
    "    decoder_outputs, _ = tf.nn.dynamic_rnn(cell=decoder_cell,\n",
    "                          inputs=emb_dec_inputs, \n",
    "                          initial_state= context,\n",
    "                          sequence_length= seq_len(decoder_inputs),\n",
    "                          scope='decoder_1')\n",
    "                         \n",
    "    tf.get_variable_scope().reuse_variables()\n",
    "    \n",
    "    decoder_outputs_inf, _ = inference_decoder(decoder_cell, context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logits and Probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Wo = tf.get_variable('Wo', shape=[dec_hdim, vocab_size], dtype=tf.float32, \n",
    "                         initializer=xinit())\n",
    "bo = tf.get_variable('bo', shape=[vocab_size], dtype=tf.float32, \n",
    "                         initializer=xinit())\n",
    "proj_outputs = tf.matmul(tf.reshape(decoder_outputs, [-1, dec_hdim]), Wo) + bo\n",
    "proj_outputs_inf = tf.matmul(tf.reshape(decoder_outputs_inf, [-1, dec_hdim]), Wo) + bo\n",
    "\n",
    "logits = tf.cond(tf.random_normal(shape=()) > 0.,\n",
    "    lambda : tf.reshape(proj_outputs, [batch_size, L, vocab_size]),\n",
    "    lambda : tf.reshape(proj_outputs_inf, [batch_size, L, vocab_size])\n",
    "                )\n",
    "probs = tf.nn.softmax(tf.reshape(proj_outputs_inf, [batch_size, L, vocab_size]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "    logits =  logits,\n",
    "    labels = targets)\n",
    "# apply mask\n",
    "masked_cross_entropy = cross_entropy * padding_mask\n",
    "# average across sequence, batch\n",
    "loss = tf.reduce_mean(masked_cross_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizatin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.01)\n",
    "train_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prediction = tf.argmax(probs, axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "config = tf.ConfigProto(allow_soft_placement = True)\n",
    "sess = tf.InteractiveSession(config = config)\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_epochs = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },

   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.30 : 3.837858788172404\n",
      "0.60 : 3.383855930964152\n",
      "0.90 : 3.2671606063842775\n",
      "0.120 : 3.2251996755599976\n",
      "0.150 : 3.1341079076131186\n",
      "0.180 : 2.8344780445098876\n",
      "0.210 : 2.7087831258773805\n",
      "0.240 : 2.5455461581548056\n",
      "0.270 : 2.5161593357721963\n",
      "0.300 : 2.704530429840088\n",
      "0.330 : 2.8626355012257894\n",
      "0.360 : 2.7578340689341228\n",
      "0.390 : 2.352905104557673\n",
      "0.420 : 2.3318983534971873\n",
      "0.450 : 2.0573596179485323\n",
      "0.480 : 2.25909686088562\n",
      "0.510 : 2.3025857249895734\n",
      "0.540 : 2.302213517824809\n",
      "0.570 : 2.3640402019023896\n",
      "0.600 : 2.636339034636815\n",
      "0.630 : 2.00689182082812\n",
      "0.660 : 2.268036327759425\n",
      "0.690 : 1.6376649141311646\n",
      "0.720 : 2.750300791859627\n",
      "0.750 : 2.253360719482104\n",
      "0.780 : 2.2311311185359957\n",
      "0.810 : 1.8239022672176362\n",
      "0.840 : 2.321496440966924\n",
      "0.870 : 2.3026630620161694\n",
      "0.900 : 1.8163844913244247\n",
      "0.930 : 2.442852649092674\n",
      "0.960 : 2.7132147083679836\n",
      "0.990 : 2.0559385935465495\n",
      "0.1020 : 2.6626084287961325\n",
      "1.30 : 1.8303838888804118\n",
      "1.60 : 2.2972772508859634\n",
      "1.90 : 1.6693303217490514\n",
      "1.120 : 2.072032004594803\n",
      "1.150 : 2.482494823137919\n",
      "1.180 : 2.152409615119298\n",
      "1.210 : 2.572398895025253\n",
      "1.240 : 2.1776503016551336\n",
      "1.270 : 2.2296440064907075\n",
      "1.300 : 2.469565379619598\n",
      "1.330 : 1.7832234144210815\n",
      "1.360 : 1.970410124460856\n",
      "1.390 : 2.291415494680405\n",
      "1.420 : 2.1271160701910654\n",
      "1.450 : 1.9733375191688538\n",
      "1.480 : 2.138724783062935\n",
      "1.510 : 2.322462542851766\n",
      "1.540 : 2.2600347965955736\n",
      "1.570 : 2.210109773278236\n",
      "1.600 : 1.279110328356425\n",
      "1.630 : 2.209134988983472\n",
      "1.660 : 2.0554347008466722\n",
      "1.690 : 1.8593452602624894\n",
      "1.720 : 2.011525101463\n",
      "1.750 : 2.1692509909470874\n",
      "1.780 : 2.5273612280686697\n",
      "1.810 : 1.8372660279273987\n",
      "1.840 : 1.7753253032763798\n",
      "1.870 : 2.9791213512420653\n",
      "1.900 : 1.9688483119010924\n",
      "1.930 : 2.04319551885128\n",
      "1.960 : 1.2624249388774236\n",
      "1.990 : 2.3597141643365225\n",
      "1.1020 : 2.385899876554807\n",
      "2.30 : 2.282043010989825\n",
      "2.60 : 1.4012788484493892\n",
      "2.90 : 2.347933061917623\n",
      "2.120 : 2.9121982236703237\n",
      "2.150 : 2.1889882614215215\n",
      "2.180 : 2.259027400612831\n",
      "2.210 : 2.198791573445002\n",
      "2.240 : 2.1552619447310764\n",
      "2.270 : 1.9757817298173905\n",
      "2.300 : 1.739817316333453\n",
      "2.330 : 2.4971760471661884\n",
      "2.360 : 1.7125539660453797\n",
      "2.390 : 2.143965111176173\n",
      "2.420 : 2.0995229204495747\n",
      "2.450 : 1.9610605597496034\n",
      "2.480 : 2.1332669297854108\n",
      "2.510 : 2.685823929309845\n",
      "2.540 : 2.0091951181491217\n",
      "2.570 : 2.068294369180997\n",
      "2.600 : 2.1553995341062544\n",
      "2.630 : 2.412194965283076\n",
      "2.660 : 1.76973546842734\n",
      "2.690 : 1.8454403708378473\n",
      "2.720 : 1.6572080274422964\n",
      "2.750 : 2.152304219206174\n",
      "2.780 : 1.402622972925504\n",
      "2.810 : 2.546757125854492\n",
      "2.840 : 2.5217205286026\n",
      "2.870 : 2.1366002559661865\n",
      "2.900 : 1.8625794539848963\n",
      "2.930 : 2.2762972633043925\n",
      "2.960 : 2.818504099051158\n",
      "2.990 : 2.2252722154061\n",
      "2.1020 : 2.133333311478297\n",
      "3.30 : 2.6344229410092037\n",
      "3.60 : 2.255904910961787\n",
      "3.90 : 1.7513887017965317\n",
      "3.120 : 2.095220546921094\n",
      "3.150 : 1.9725952784220377\n",
      "3.180 : 2.2536642263333\n",
      "3.210 : 2.087851726015409\n",
      "3.240 : 1.7736863195896149\n",
      "3.270 : 2.0942852397759757\n",
      "3.300 : 2.199061228831609\n",
      "3.330 : 2.2451971262693404\n",
      "3.360 : 2.06055892209212\n",
      "3.390 : 2.5299066066741944\n",
      "3.420 : 2.118745399514834\n",
      "3.450 : 1.842803848783175\n",
      "3.480 : 2.2325301776329676\n",
      "3.510 : 2.3017826010783513\n",
      "3.540 : 2.4884498437245686\n",
      "3.570 : 2.2902186582485835\n",
      "3.600 : 1.6302834341923396\n",
      "3.630 : 1.4747334450483323\n",
      "3.660 : 2.132566924889882\n",
      "3.690 : 2.5801176855961483\n",
      "3.720 : 2.09182087679704\n",
      "3.750 : 2.272305295864741\n",
      "3.780 : 2.642225475112597\n",
      "3.810 : 1.946841358145078\n",
      "3.840 : 1.7599405815203986\n",
      "3.870 : 2.106056253115336\n",
      "3.900 : 2.460927172501882\n",
      "3.930 : 2.270557649930318\n",
      "3.960 : 2.0852784276008607\n",
      "3.990 : 2.367138143380483\n",
      "3.1020 : 2.362760092814763\n",
      "4.30 : 1.6528267840544382\n",
      "4.60 : 1.8666561911503474\n",
      "4.90 : 1.9701807955900827\n",
      "4.120 : 2.3207809070746106\n",
      "4.150 : 2.7975583344697954\n",
      "4.180 : 2.012466865777969\n",
      "4.210 : 1.6080683022737503\n",
      "4.240 : 2.4064939588308336\n",
      "4.270 : 1.6243735472361247\n",
      "4.300 : 2.6848690817753473\n",
      "4.330 : 2.4952466577291488\n",
      "4.360 : 1.590386148293813\n",
      "4.390 : 1.6585638354221979\n",
      "4.420 : 2.480360579490662\n",
      "4.450 : 1.8335332095623016\n",
      "4.480 : 1.7484148104985555\n",
      "4.510 : 1.827934276064237\n",
      "4.540 : 1.7321966171264649\n",
      "4.570 : 1.9302261342604956\n",
      "4.600 : 2.150562920173009\n",
      "4.630 : 2.168983822067579\n",
      "4.660 : 2.367963186899821\n",
      "4.690 : 2.78194219271342\n",
      "4.720 : 1.3057983458042144\n",
      "4.750 : 2.4218159059683484\n",
      "4.780 : 2.516574430465698\n",
      "4.810 : 1.9475728174050648\n",
      "4.840 : 2.4098366051912308\n",
      "4.870 : 2.589337713519732\n",
      "4.900 : 2.218873769044876\n",
      "4.930 : 2.6311735490957897\n",
      "4.960 : 2.3155683845281603\n",
      "4.990 : 1.523283804456393\n",
      "4.1020 : 2.2551894088586173\n",
      "5.30 : 2.1353549748659133\n",
      "5.60 : 1.8762255549430846\n",
      "5.90 : 2.117699251572291\n",
      "5.120 : 1.9677845140298207\n",
      "5.150 : 1.9629893054564793\n",
      "5.180 : 2.359323489665985\n",
      "5.210 : 1.3482088138659796\n",
      "5.240 : 1.8806358138720194\n",
      "5.270 : 2.3387586265802383\n",
      "5.300 : 2.3332027971744536\n",
      "5.330 : 2.1226827601591745\n",
      "5.360 : 1.9359615017970404\n",
      "5.390 : 2.3793525884548825\n",
      "5.420 : 2.4666614532470703\n",
      "5.450 : 2.1749858339627584\n",
      "5.480 : 2.2154516090949374\n",
      "5.510 : 1.8414070775111517\n",
      "5.540 : 2.3623483190933863\n",
      "5.570 : 1.816482491294543\n",
      "5.600 : 2.499298032124837\n",
      "5.630 : 2.182275985678037\n",
      "5.660 : 2.6202595820029577\n",
      "5.690 : 1.9684352884689966\n",
      "5.720 : 2.3441933085521063\n",
      "5.750 : 2.0216946085294087\n",
      "5.780 : 2.1374172111352285\n",
      "5.810 : 1.8316801697015763\n",
      "5.840 : 1.7622145146131516\n",
      "5.870 : 2.3551937103271485\n",
      "5.900 : 2.2158531576395033\n",
      "5.930 : 1.8886042932669322\n",
      "5.960 : 1.6065028031667075\n",
      "5.990 : 2.488270475467046\n",
      "5.1020 : 2.24140344162782\n",
      "6.30 : 2.5101413001616795\n",
      "6.60 : 2.7192130664984386\n",
      "6.90 : 2.3198160966237387\n",
      "6.120 : 2.0817200938860574\n",
      "6.150 : 2.3252498189608257\n",
      "6.180 : 1.9914427936077117\n",
      "6.210 : 1.7316571414470672\n",
      "6.240 : 2.883722980817159\n",
      "6.270 : 2.565665143728256\n",
      "6.300 : 2.094678849975268\n",
      "6.330 : 1.9877481182416281\n",
      "6.360 : 2.198146928350131\n",
      "6.390 : 2.1755249917507173\n",
      "6.420 : 1.8488908956448238\n",
      "6.450 : 1.9569871226946514\n",
      "6.480 : 2.462340474128723\n",
      "6.510 : 2.1808970580498377\n",
      "6.540 : 2.495955416560173\n",
      "6.570 : 2.172944497068723\n",
      "6.600 : 2.115676321585973\n",
      "6.630 : 2.1790324678023656\n",
      "6.660 : 2.258489556113879\n",
      "6.690 : 2.2143860350052518\n",
      "6.720 : 1.644625836610794\n",
      "6.750 : 2.2596540341774625\n",
      "6.780 : 2.014391941825549\n",
      "6.810 : 2.2036397576332094\n",
      "6.840 : 2.2563636203606925\n",
      "6.870 : 2.113421662648519\n",
      "6.900 : 1.8382551123698552\n",
      "6.930 : 1.911769617597262\n",
      "6.960 : 2.3341563820838926\n",
      "6.990 : 1.9807467808326085\n",
      "6.1020 : 2.249027505517006\n",
      "7.30 : 2.5283314148585\n",
      "7.60 : 2.1165586918592454\n",
      "7.90 : 2.089248399933179\n",
      "7.120 : 2.5487240155537925\n",
      "7.150 : 2.1902329047520954\n",
      "7.180 : 1.993045727411906\n",
      "7.210 : 1.8344222505887349\n",
      "7.240 : 2.521770276625951\n",
      "7.270 : 2.3299372464418413\n",
      "7.300 : 1.7157731612523397\n",
      "7.330 : 2.365069063504537\n",
      "7.360 : 1.943149251739184\n",
      "7.390 : 1.9046464840571085\n",
      "7.420 : 2.2322976181904477\n",
      "7.450 : 2.201129107673963\n",
      "7.480 : 2.106745942433675\n",
      "7.510 : 1.9292442907889684\n",
      "7.540 : 2.0059436241785686\n",
      "7.570 : 2.1764682829380035\n",
      "7.600 : 1.640656848748525\n",
      "7.630 : 1.8089642186959585\n",
      "7.660 : 2.2451975365479786\n",
      "7.690 : 2.324701847632726\n",
      "7.720 : 1.999050322175026\n",
      "7.750 : 2.0098609209060667\n",
      "7.780 : 2.158420595526695\n",
      "7.810 : 2.077330614129702\n",
      "7.840 : 2.397077089548111\n",
      "7.870 : 2.360125333070755\n",
      "7.900 : 1.8444215267896653\n",
      "7.930 : 1.775286681453387\n",
      "7.960 : 1.963041944305102\n",
      "7.990 : 2.1990183065334956\n",
      "7.1020 : 2.6098484774430593\n",
      "8.30 : 2.647840830683708\n",
      "8.60 : 1.8762466897567114\n",
      "8.90 : 2.5787255724271136\n",
      "8.120 : 2.659355535109838\n",
      "8.150 : 1.9370330591996512\n",
      "8.180 : 1.6108593702316285\n",
      "8.210 : 2.1770970940589907\n",
      "8.240 : 1.9014992892742157\n",
      "8.270 : 1.8566627283891042\n",
      "8.300 : 2.2042912552754084\n",
      "8.330 : 2.4704386701186496\n",
      "8.360 : 1.8258877237637838\n",
      "8.390 : 2.267176220814387\n",
      "8.420 : 2.345020438234011\n",
      "8.450 : 1.8471008211374282\n",
      "8.480 : 2.34102234741052\n",
      "8.510 : 1.7126798063516617\n",
      "8.540 : 2.8546543886264164\n",
      "8.570 : 1.830072021484375\n",
      "8.600 : 2.248400405049324\n",
      "8.630 : 1.932454800605774\n",
      "8.660 : 2.1114637782176335\n",
      "8.690 : 1.6015490661064784\n",
      "8.720 : 1.6283996870120367\n",
      "8.750 : 1.8962471077839533\n",
      "8.780 : 2.5024830172459285\n",
      "8.810 : 2.3084583461284636\n",
      "8.840 : 2.2341223508119583\n",
      "8.870 : 1.8834556569655736\n",
      "8.900 : 1.7070019245147705\n",
      "8.930 : 2.515431733926137\n",
      "8.960 : 1.8154393235842388\n",
      "8.990 : 2.1427234490712483\n",
      "8.1020 : 2.500291230281194\n",
      "9.30 : 2.2414493103822073\n",
      "9.60 : 1.9849247346321741\n",
      "9.90 : 2.5806619157393773\n",
      "9.120 : 2.41243234872818\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.150 : 3.0444020450115206\n",
      "9.180 : 1.5025496939818064\n",
      "9.210 : 1.8533472816149394\n",
      "9.240 : 1.8896602123975754\n",
      "9.270 : 2.4663062751293183\n",
      "9.300 : 1.7038935621579487\n",
      "9.330 : 2.3522899210453034\n",
      "9.360 : 1.5867762118577957\n",
      "9.390 : 2.1164768735567727\n",
      "9.420 : 1.9918962299823761\n",
      "9.450 : 2.5462564527988434\n",
      "9.480 : 1.8669273406267166\n",
      "9.510 : 2.174719689289729\n",
      "9.540 : 1.7534892588853837\n",
      "9.570 : 2.0472425570090613\n",
      "9.600 : 2.0113101770480473\n",
      "9.630 : 2.642964693903923\n",
      "9.660 : 2.5092757592598596\n",
      "9.690 : 2.188135651747386\n",
      "9.720 : 2.5367728650569914\n",
      "9.750 : 2.1493782669305803\n",
      "9.780 : 2.3834354122479757\n",
      "9.810 : 2.4204405317703883\n",
      "9.840 : 1.514064144094785\n",
      "9.870 : 2.4908126840988793\n",
      "9.900 : 1.4957039028406143\n",
      "9.930 : 2.0033007591962813\n",
      "9.960 : 2.585219637552897\n",
      "9.990 : 2.128259265422821\n",
      "9.1020 : 1.8827596257130306\n",
      "10.30 : 1.990147837003072\n",
      "10.60 : 2.4613777667284014\n",
      "10.90 : 2.2107350806395214\n",
      "10.120 : 1.6999508221944173\n",
      "10.150 : 2.2066848307847975\n",
      "10.180 : 1.7455294887224833\n",
      "10.210 : 1.7160188535849252\n",
      "10.240 : 2.1266936123371125\n",
      "10.270 : 2.236362487077713\n",
      "10.300 : 2.316029268503189\n",
      "10.330 : 1.5011991947889327\n",
      "10.360 : 2.0589335352182387\n",
      "10.390 : 2.0101707418759664\n",
      "10.420 : 2.477408065398534\n",
      "10.450 : 2.6681872884432476\n",
      "10.480 : 2.103236378232638\n",
      "10.510 : 1.70051815311114\n",
      "10.540 : 2.853588154911995\n",
      "10.570 : 2.271335975329081\n",
      "10.600 : 2.119198536872864\n",
      "10.630 : 1.451438053448995\n",
      "10.660 : 3.0930407484372457\n",
      "10.690 : 2.533175051212311\n",
      "10.720 : 1.3200778563817341\n",
      "10.750 : 2.1645621995131177\n",
      "10.780 : 2.390954832235972\n",
      "10.810 : 2.291178830464681\n",
      "10.840 : 2.0011274755001067\n",
      "10.870 : 2.235957419872284\n",
      "10.900 : 1.965470752120018\n",
      "10.930 : 2.1324709673722584\n",
      "10.960 : 2.3351947496334713\n",
      "10.990 : 1.9714905907710394\n",
      "10.1020 : 2.115213084220886\n",
      "11.30 : 2.035937626163165\n",
      "11.60 : 2.3646120419104895\n",
      "11.90 : 2.456435524423917\n",
      "11.120 : 2.197254917025566\n",
      "11.150 : 1.6248069643974303\n",
      "11.180 : 2.1163735280434293\n",
      "11.210 : 2.7778235346078874\n",
      "11.240 : 1.8995407571395238\n",
      "11.270 : 2.084926665822665\n",
      "11.300 : 1.9741631199916203\n",
      "11.330 : 2.3663936873277027\n",
      "11.360 : 2.067361862460772\n",
      "11.390 : 2.6358637432257335\n",
      "11.420 : 2.479967733224233\n",
      "11.450 : 2.6819392293691635\n",
      "11.480 : 2.586943271756172\n",
      "11.510 : 1.8408864150444666\n",
      "11.540 : 2.834036547938983\n",
      "11.570 : 1.9350406408309937\n",
      "11.600 : 1.6211821715037027\n",
      "11.630 : 2.1873157024383545\n",
      "11.660 : 1.8968643893798192\n",
      "11.690 : 2.0799865454435347\n",
      "11.720 : 2.2344536652167637\n",
      "11.750 : 2.5127617180347444\n",
      "11.780 : 1.7465986649195353\n",
      "11.810 : 1.6991100450356802\n",
      "11.840 : 1.7673682391643524\n",
      "11.870 : 2.1240297347307204\n",
      "11.900 : 1.851903998851776\n",
      "11.930 : 1.3972077618042629\n",
      "11.960 : 2.3248283356428145\n",
      "11.990 : 2.3490340530872347\n",
      "11.1020 : 2.005887344479561\n",
      "12.30 : 1.8994095077117283\n",
      "12.60 : 2.5901593546072643\n",
      "12.90 : 1.9661571681499481\n",
      "12.120 : 2.31162707110246\n",
      "12.150 : 1.8251671264568965\n",
      "12.180 : 2.2344458937644958\n",
      "12.210 : 1.9525633484125138\n",
      "12.240 : 1.7616716275612514\n",
      "12.270 : 2.092648281653722\n",
      "12.300 : 1.726256150007248\n",
      "12.330 : 1.7507263044516246\n",
      "12.360 : 2.0601479530334474\n",
      "12.390 : 2.1378927727540336\n",
      "12.420 : 2.3367877125740053\n",
      "12.450 : 1.9452425996462503\n",
      "12.480 : 1.7499892910321553\n",
      "12.510 : 2.182454611857732\n",
      "12.540 : 2.2546394556760787\n",
      "12.570 : 1.80750705798467\n",
      "12.600 : 1.872311704357465\n",
      "12.630 : 2.292535354693731\n",
      "12.660 : 2.3675682713588078\n",
      "12.690 : 2.307516454656919\n",
      "12.720 : 1.6419601420561472\n",
      "12.750 : 1.5166450272003809\n",
      "12.780 : 2.2435060570637386\n",
      "12.810 : 2.197926758726438\n",
      "12.840 : 1.8996862183014551\n",
      "12.870 : 2.1104409635066985\n",
      "12.900 : 2.4306566129128138\n",
      "12.930 : 2.259598061442375\n",
      "12.960 : 2.088749052087466\n",
      "12.990 : 2.1245695163806277\n",
      "12.1020 : 2.251255352298419\n",
      "13.30 : 2.139027750492096\n",
      "13.60 : 1.8604365706443786\n",
      "13.90 : 2.3523694266875586\n",
      "13.120 : 2.2862585961818693\n",
      "13.150 : 2.088130143284798\n",
      "13.180 : 2.9512976119915644\n",
      "13.210 : 2.204485547542572\n",
      "13.240 : 2.0210115174452463\n",
      "13.270 : 1.970004963874817\n",
      "13.300 : 2.339712980389595\n",
      "13.330 : 2.4874042282501856\n",
      "13.360 : 1.7087295303742092\n",
      "13.390 : 2.394856890042623\n",
      "13.420 : 1.8623488336801528\n",
      "13.450 : 2.440502569079399\n",
      "13.480 : 2.0986053546269736\n",
      "13.510 : 2.9016420880953473\n",
      "13.540 : 1.7543198019266129\n",
      "13.570 : 2.294813346862793\n",
      "13.600 : 2.350056795279185\n",
      "13.630 : 1.8245672762393952\n",
      "13.660 : 2.017179091771444\n",
      "13.690 : 1.9378864079713822\n",
      "13.720 : 1.7459848463535308\n",
      "13.750 : 1.6513081123431523\n",
      "13.780 : 2.2427733620007833\n",
      "13.810 : 1.9339721828699112\n",
      "13.840 : 2.745647080739339\n",
      "13.870 : 2.1251566002766293\n",
      "13.900 : 1.8547492305437723\n",
      "13.930 : 2.0191100627183913\n",
      "13.960 : 2.0986181130011876\n",
      "13.990 : 1.380613695581754\n",
      "13.1020 : 1.9974803725878398\n",
      "14.30 : 2.402428288261096\n",
      "14.60 : 2.226043798526128\n",
      "14.90 : 1.9802486975987752\n",
      "14.120 : 2.4298689693212507\n",
      "14.150 : 2.308614389101664\n",
      "14.180 : 2.245841160416603\n",
      "14.210 : 1.7029802292585372\n",
      "14.240 : 1.7404336899518966\n",
      "14.270 : 2.4510197808345158\n",
      "14.300 : 1.8604333788156509\n",
      "14.330 : 2.733037586013476\n",
      "14.360 : 2.1878240168094636\n",
      "14.390 : 2.3999117016792297\n",
      "14.420 : 1.9802586336930592\n",
      "14.450 : 2.0777216305335364\n",
      "14.480 : 2.2124409367640814\n",
      "14.510 : 2.2983564684788385\n",
      "14.540 : 2.363591648141543\n",
      "14.570 : 1.818023474017779\n",
      "14.600 : 2.368230030934016\n",
      "14.630 : 2.401135344306628\n",
      "14.660 : 1.8887893786032994\n",
      "14.690 : 2.3101417014996213\n",
      "14.720 : 2.5296910127003986\n",
      "14.750 : 2.2760030577580133\n",
      "14.780 : 1.7718412001927695\n",
      "14.810 : 2.0656047850847243\n",
      "14.840 : 1.899901220202446\n",
      "14.870 : 1.6375886003176372\n",
      "14.900 : 1.985792690515518\n",
      "14.930 : 2.004552313685417\n",
      "14.960 : 1.7335252632697424\n",
      "14.990 : 2.8576058397690454\n",
      "14.1020 : 2.1233680258194605\n",
      "15.30 : 2.8628626932700474\n",
      "15.60 : 1.880313362677892\n",
      "15.90 : 1.2670934279759725\n",
      "15.120 : 2.1914114902416864\n",
      "15.150 : 2.307945038874944\n",
      "15.180 : 2.3731077551841735\n",
      "15.210 : 1.705181117852529\n",
      "15.240 : 2.397316802541415\n",
      "15.270 : 1.4992965082327525\n",
      "15.300 : 2.067038266857465\n",
      "15.330 : 1.8591126749912898\n",
      "15.360 : 2.4141076733668645\n",
      "15.390 : 2.148442687590917\n",
      "15.420 : 2.4707827627658845\n",
      "15.450 : 2.1800841550032297\n",
      "15.480 : 2.461626417438189\n",
      "15.510 : 1.7222340216239294\n",
      "15.540 : 2.2312840978304544\n",
      "15.570 : 2.2853734105825425\n",
      "15.600 : 1.9999252289533616\n",
      "15.630 : 1.8136579424142838\n",
      "15.660 : 1.8524773329496385\n",
      "15.690 : 2.0878535310427346\n",
      "15.720 : 2.1029159714778265\n",
      "15.750 : 2.2703813672065736\n",
      "15.780 : 1.7557392835617065\n",
      "15.810 : 3.128415676951408\n",
      "15.840 : 2.0026643653710683\n",
      "15.870 : 2.1265995770692827\n",
      "15.900 : 2.338362748424212\n",
      "15.930 : 1.5314785689115524\n",
      "15.960 : 1.8536840766668319\n",
      "15.990 : 1.878547810514768\n",
      "15.1020 : 2.0204648206631344\n",
      "16.30 : 2.2575871378183363\n",
      "16.60 : 1.7576921880245209\n",
      "16.90 : 1.829682496190071\n",
      "16.120 : 1.9696121007204055\n",
      "16.150 : 2.819596434632937\n",
      "16.180 : 2.596340059240659\n",
      "16.210 : 2.075252746542295\n",
      "16.240 : 2.0370132674773536\n",
      "16.270 : 1.6116501986980438\n",
      "16.300 : 2.099781167507172\n",
      "16.330 : 2.1136481751998266\n",
      "16.360 : 2.1776680449644723\n",
      "16.390 : 2.1515477736790976\n",
      "16.420 : 2.3641691356897354\n",
      "16.450 : 1.7046972393989563\n",
      "16.480 : 1.9550808678070704\n",
      "16.510 : 2.2043377667665482\n",
      "16.540 : 2.4753628114859265\n",
      "16.570 : 2.0669425815343856\n",
      "16.600 : 2.1057181497414907\n",
      "16.630 : 2.5306879073381423\n",
      "16.660 : 1.630086941520373\n",
      "16.690 : 2.305281518896421\n",
      "16.720 : 2.110443600018819\n",
      "16.750 : 2.137855946024259\n",
      "16.780 : 2.005075780550639\n",
      "16.810 : 2.048319412271182\n",
      "16.840 : 3.0353806257247924\n",
      "16.870 : 2.230669092138608\n",
      "16.900 : 1.7279641340176264\n",
      "16.930 : 2.3876991083224612\n",
      "16.960 : 2.069034445285797\n",
      "16.990 : 2.1229375938574475\n",
      "16.1020 : 2.592918993035952\n",
      "17.30 : 2.1525789578755696\n",
      "17.60 : 2.100756903489431\n",
      "17.90 : 1.8406180848677953\n",
      "17.120 : 2.4146510740121205\n",
      "17.150 : 1.8518874049186707\n",
      "17.180 : 2.60862286289533\n",
      "17.210 : 1.9610870043436686\n",
      "17.240 : 1.776486274600029\n",
      "17.270 : 2.2136787960926694\n",
      "17.300 : 2.1185766498247784\n",
      "17.330 : 1.8799470653136572\n",
      "17.360 : 2.17173625032107\n",
      "17.390 : 2.140883782505989\n",
      "17.420 : 2.814562381307284\n",
      "17.450 : 2.1999410311381022\n",
      "17.480 : 2.2316215376059216\n",
      "17.510 : 2.294162021080653\n",
      "17.540 : 1.8546145349740981\n",
      "17.570 : 2.868914624055227\n",
      "17.600 : 1.8691047022740046\n",
      "17.630 : 1.6955205827951432\n",
      "17.660 : 1.6423052261273066\n",
      "17.690 : 2.5454255123933156\n",
      "17.720 : 1.8655183384815852\n",
      "17.750 : 1.7576143046220143\n",
      "17.780 : 2.2502987504005434\n",
      "17.810 : 2.178993099927902\n",
      "17.840 : 2.1340143849452335\n",
      "17.870 : 1.7370664715766906\n",
      "17.900 : 2.206483227014542\n",
      "17.930 : 2.1371308902899426\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17.960 : 1.460916605591774\n",
      "17.990 : 2.32221071322759\n",
      "17.1020 : 2.2495528678099315\n",
      "18.30 : 2.2675901661316553\n",
      "18.60 : 2.2388717750708262\n",
      "18.90 : 1.9838566521803538\n",
      "18.120 : 1.5920667986075083\n",
      "18.150 : 2.195311736067136\n",
      "18.180 : 1.7599268148342768\n",
      "18.210 : 1.8395989805459976\n",
      "18.240 : 2.279437534014384\n",
      "18.270 : 1.854657448331515\n",
      "18.300 : 2.160920383532842\n",
      "18.330 : 2.48043150305748\n",
      "18.360 : 2.169153144955635\n",
      "18.390 : 2.1558420171340305\n",
      "18.420 : 1.8636635740598042\n",
      "18.450 : 1.8459152807792027\n",
      "18.480 : 2.0761125991741816\n",
      "18.510 : 2.4129818240801493\n",
      "18.540 : 1.3876755674680075\n",
      "18.570 : 1.582807895541191\n",
      "18.600 : 1.8803165157636006\n",
      "18.630 : 2.184755575656891\n",
      "18.660 : 2.1266051401694614\n",
      "18.690 : 1.843825215101242\n",
      "18.720 : 1.768381623427073\n",
      "18.750 : 2.140452847878138\n",
      "18.780 : 2.022594836354256\n",
      "18.810 : 2.2965122838815053\n",
      "18.840 : 2.112008686860402\n",
      "18.870 : 1.6199007004499435\n",
      "18.900 : 2.0935059050718943\n",
      "18.930 : 2.2585726777712503\n",
      "18.960 : 2.2255122631788256\n",
      "18.990 : 1.7496970256169637\n",
      "18.1020 : 1.7406081428130469\n",
      "19.30 : 2.009610973795255\n",
      "19.60 : 1.627598562836647\n",
      "19.90 : 2.3378278871377307\n",
      "19.120 : 2.3010405162970224\n",
      "19.150 : 2.0929581662019094\n",
      "19.180 : 2.001862234870593\n",
      "19.210 : 1.7155706723531088\n",
      "19.240 : 2.0061865876118343\n",
      "19.270 : 1.969620943069458\n",
      "19.300 : 2.080100442965825\n",
      "19.330 : 2.353775982062022\n",
      "19.360 : 1.335619071125984\n",
      "19.390 : 2.020095952351888\n",
      "19.420 : 2.1168310085932416\n",
      "19.450 : 2.2041130026181537\n",
      "19.480 : 1.6157146205504735\n",
      "19.510 : 1.9306874603033066\n",
      "19.540 : 1.9897836258014043\n",
      "19.570 : 1.939595494667689\n",
      "19.600 : 2.016622433066368\n",
      "19.630 : 1.4753141969442367\n",
      "19.660 : 2.2551645090182624\n",
      "19.690 : 2.426533169547717\n",
      "19.720 : 2.5459688703219094\n",
      "19.750 : 1.9082817415396371\n",
      "19.780 : 1.77326251467069\n",
      "19.810 : 1.949087588985761\n",
      "19.840 : 2.245986644426982\n",
      "19.870 : 1.8613125781218212\n",
      "19.900 : 1.838947461048762\n",
      "19.930 : 2.7594288289546967\n",
      "19.960 : 2.4412136018276214\n",
      "19.990 : 1.9972470690806707\n",
      "19.1020 : 2.0016875565052032\n"

     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-b3f69adef234>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m         _, loss_v = sess.run([train_op, loss], feed_dict = {\n\u001b[1;32m      5\u001b[0m             \u001b[0minputs\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0midx_q\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m             \u001b[0mtargets\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0midx_a\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         })\n\u001b[1;32m      8\u001b[0m         \u001b[0mavg_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss_v\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/suriyadeepan/Desktop/env/tf/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    776\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 778\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    779\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/suriyadeepan/Desktop/env/tf/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    980\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 982\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    983\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/suriyadeepan/Desktop/env/tf/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1030\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1032\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1033\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/home/suriyadeepan/Desktop/env/tf/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1037\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1040\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/suriyadeepan/Desktop/env/tf/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1019\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1020\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(num_epochs):\n",
    "    avg_loss = 0.\n",
    "    for j in range(len(idx_q)//B):\n",
    "        _, loss_v = sess.run([train_op, loss], feed_dict = {\n",
    "            inputs : idx_q[j*B:(j+1)*B],\n",
    "            targets : idx_a[j*B:(j+1)*B]\n",
    "        })\n",
    "        avg_loss += loss_v\n",
    "        if j and j%30==0:\n",
    "            print('{}.{} : {}'.format(i,j,avg_loss/30))\n",
    "            avg_loss = 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare CE and masked CE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ce, mce = sess.run([cross_entropy, masked_cross_entropy], feed_dict = {\n",
    "            inputs : idx_q[j*B:(j+1)*B],\n",
    "            targets : idx_a[j*B:(j+1)*B]\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ce[0], mce[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "j = 117\n",
    "pred_v = sess.run(prediction, feed_dict = {\n",
    "            inputs : idx_q[j*B:(j+1)*B],\n",
    "            #targets : idx_a[j*B:(j+1)*B]\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred_v[0], idx_a[j*B:(j+1)*B][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def arr2sent(arr):\n",
    "    return ' '.join([i2w[item] for item in arr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n"
     ]
    }
   ],
   "source": [
    "for i in range(B):\n",
    "    print(arr2sent(pred_v[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sexist women are saying the shit also dumb ass unk smh EOS _ _ _ _ _ _ _ _ _'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr2sent( idx_a[j*B:(j+1)*B][11])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
